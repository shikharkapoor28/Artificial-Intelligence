ğŸ§  Gemini Memory Chatbot (Console)

A memory-augmented conversational AI chatbot built using Googleâ€™s Gemini 1.5 Flash model.
This project demonstrates how to simulate long-term conversational memory using LLM-based recursive summarization, allowing the chatbot to retain context across multiple turns in a console environment.

ğŸš€ What This Project Does

Unlike a basic prompt-response chatbot, this system:

Maintains conversation memory

Compresses past dialogue into a rolling summary

Feeds that summary back into every new model prompt

Allows Gemini to reason over prior context

This creates the illusion of a persistent, intelligent assistant that â€œremembersâ€ what was said earlier.

ğŸ§© Architecture
User Input
   â†“
Conversation Summary (Memory)
   â†“
Prompt = [Memory + New Message]
   â†“
Gemini 1.5 Flash
   â†“
Response
   â†“
Update Memory via LLM Summarization
   â†º (loop)


This design mimics how production AI systems (ChatGPT, Claude, Copilot) maintain conversational context under token limits.

ğŸ§  Key Features

Persistent Memory
Uses Gemini to summarize the entire conversation after every turn, allowing unlimited chat length.

Context Injection
The chatbot prepends memory to every prompt so responses remain relevant.

Stateless API â†’ Stateful Agent
Converts Geminiâ€™s stateless API into a stateful conversational agent.

Console Interface
Simple, fast, and ideal for experimentation and learning.

ğŸ› ï¸ Tech Stack

Model: Gemini 1.5 Flash

Language: Python

SDK: Google Generative AI Python SDK

ğŸ“¦ Installation
pip install google-generativeai

ğŸ”‘ Setup

Set your Gemini API key:

os.environ["API_KEY"] = "YOUR_GEMINI_API_KEY"


Or export it in your shell:

export API_KEY="YOUR_GEMINI_API_KEY"

â–¶ï¸ Run the Chatbot
python chatbot.py


Type your messages and type exit to quit.

ğŸ§  Example
You: My name is Shikhar
Gemini: Nice to meet you, Shikhar!

You: What is my name?
Gemini: Your name is Shikhar.


This works because the chatbot remembers via summarization.

ğŸ§ª Why This Matters

This project demonstrates:

How LLMs can be given memory

How to build RAG-style conversational agents

How to overcome context window limits

How modern AI assistants work internally

This is a foundation for:

AI agents

Customer support bots

Research assistants

Chat-based applications

âš ï¸ Limitations

This implementation uses recursive summarization, which may:

Lose fine details over very long conversations

Drift slightly over time

Production systems typically replace this with:

Vector databases (FAISS, Pinecone, Chroma)

Semantic retrieval (RAG)

ğŸŒ± Future Enhancements

Vector memory store (FAISS / Chroma)

Retrieval-augmented generation (RAG)

Web or API interface

Multi-session user memory

ğŸ‘¨â€ğŸ’» Author

Shikhar Kapoor
AI Engineer | Data Scientist | Systems Builder
